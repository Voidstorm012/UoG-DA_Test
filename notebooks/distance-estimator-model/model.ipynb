{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Distance Estimator Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Setup**\n",
    "### **Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Add the project directory to the path for imports\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    \n",
    "print(f\"Added project root to sys.path: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preparation**\n",
    "### **Load and preprocess the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data preparation module\n",
    "from data_preparation import preprocess\n",
    "\n",
    "# Load and prepare the data for regression\n",
    "X_train, X_test, y_train, y_test, feature_names, raw_data, processed_data = preprocess.prepare_data('regression')\n",
    "\n",
    "# Display the shapes of the data\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"Feature names: {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for easier exploration\n",
    "df_train = pd.DataFrame(X_train, columns=feature_names)\n",
    "df_train['Range'] = y_train\n",
    "\n",
    "# Summary statistics\n",
    "print(\"Summary statistics:\")\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the target variable (Range) - WITHOUT KDE to avoid errors\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_train['Range'], kde=False)\n",
    "plt.title('Distribution of Range Values')\n",
    "plt.xlabel('Range')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range distribution by NLOS flag\n",
    "if 'NLOS' in df_train.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='NLOS', y='Range', data=df_train)\n",
    "    plt.title('Range Distribution by NLOS Flag')\n",
    "    plt.xlabel('NLOS (0=LOS, 1=NLOS)')\n",
    "    plt.ylabel('Range')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for each feature against the target variable\n",
    "num_features = len(feature_names)\n",
    "num_rows = (num_features + 1) // 2  # Calculate number of rows needed\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, 2, figsize=(15, 4 * num_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(feature_names):\n",
    "    sns.scatterplot(x=feature, y='Range', data=df_train, ax=axes[i], alpha=0.5)\n",
    "    axes[i].set_title(f'{feature} vs Range')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Range')\n",
    "    \n",
    "# Remove any unused axes\n",
    "for i in range(num_features, len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df_train.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Training and Evaluation**\n",
    "### **Train various regression models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regression models to train\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'SVR': SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "}\n",
    "\n",
    "# Standardize the features for models that need it\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Train the model (use scaled data for some models)\n",
    "    if name in ['Linear Regression', 'Ridge Regression', 'Lasso Regression', 'SVR']:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Feature importance\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance = model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        feature_importance = np.abs(model.coef_)\n",
    "    else:\n",
    "        feature_importance = None\n",
    "    \n",
    "    # Store the results and predictions\n",
    "    results[name] = {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'feature_importance': feature_importance,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"  MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualize model performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "metrics = ['mse', 'rmse', 'mae', 'r2']\n",
    "labels = ['MSE', 'RMSE', 'MAE', 'R²']\n",
    "comparison_df = pd.DataFrame({name: [results[name][metric] for metric in metrics] for name in results.keys()}, \n",
    "                             index=labels)\n",
    "\n",
    "# Plot MSE, RMSE, MAE\n",
    "plt.figure(figsize=(12, 8))\n",
    "comparison_df.iloc[:3].plot(kind='bar')\n",
    "plt.title('Model Error Metrics Comparison')\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.legend(title='Model')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Plot R²\n",
    "plt.figure(figsize=(12, 6))\n",
    "comparison_df.iloc[3:].plot(kind='bar')\n",
    "plt.title('Model R² Comparison')\n",
    "plt.ylabel('R²')\n",
    "plt.xlabel('Metric')\n",
    "plt.legend(title='Model')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted plots for each model\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, name in enumerate(results):\n",
    "    if i < len(axes):\n",
    "        axes[i].scatter(y_test, results[name]['y_pred'], alpha=0.5)\n",
    "        axes[i].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\n",
    "        axes[i].set_title(f'Actual vs Predicted - {name}')\n",
    "        axes[i].set_xlabel('Actual Range')\n",
    "        axes[i].set_ylabel('Predicted Range')\n",
    "        axes[i].text(0.05, 0.95, f\"R² = {results[name]['r2']:.3f}\\nRMSE = {results[name]['rmse']:.3f}\", \n",
    "                 transform=axes[i].transAxes, fontsize=12, verticalalignment='top')\n",
    "\n",
    "# Remove any unused axes\n",
    "for i in range(len(results), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature importance analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for best performing model (highest R²)\n",
    "best_model_name = max(results, key=lambda x: results[x]['r2'])\n",
    "print(f\"Best performing model based on R²: {best_model_name}\")\n",
    "\n",
    "if results[best_model_name]['feature_importance'] is not None:\n",
    "    # Create a DataFrame for feature importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': results[best_model_name]['feature_importance']\n",
    "    })\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "    plt.title(f'Feature Importance ({best_model_name})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Error Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis for the best model\n",
    "best_model_name = max(results, key=lambda x: results[x]['r2'])\n",
    "y_pred = results[best_model_name]['y_pred']\n",
    "errors = y_test - y_pred\n",
    "\n",
    "# Create DataFrame for error analysis\n",
    "error_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_pred,\n",
    "    'Error': errors\n",
    "})\n",
    "\n",
    "# Error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(error_df['Error'], kde=False)\n",
    "plt.title(f'Error Distribution for {best_model_name}')\n",
    "plt.xlabel('Error (Actual - Predicted)')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error vs Actual plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(error_df['Actual'], error_df['Error'], alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.title('Error vs Actual Range')\n",
    "plt.xlabel('Actual Range')\n",
    "plt.ylabel('Error')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion and Recommendations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best model\n",
    "best_model_rmse = min(results, key=lambda x: results[x]['rmse'])\n",
    "best_model_r2 = max(results, key=lambda x: results[x]['r2'])\n",
    "\n",
    "print(f\"Best model by RMSE: {best_model_rmse} ({results[best_model_rmse]['rmse']:.4f})\")\n",
    "print(f\"Best model by R²: {best_model_r2} ({results[best_model_r2]['r2']:.4f})\")\n",
    "\n",
    "# Recommendations based on the analysis\n",
    "print(\"\\nRecommendations:\")\n",
    "print(f\"1. Use {best_model_r2} for range prediction due to its high R² and low error.\")\n",
    "print(\"2. Important features for range prediction (from most to least important):\")\n",
    "if results[best_model_r2]['feature_importance'] is not None:\n",
    "    for feature, importance in importance_df.head(5).values:\n",
    "        print(f\"   - {feature}: {importance:.4f}\")\n",
    "print(\"3. Focus on these features when optimizing the range prediction model.\")\n",
    "print(f\"4. Error analysis shows that the model has a mean error of {errors.mean():.4f} with a standard deviation of {errors.std():.4f}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}